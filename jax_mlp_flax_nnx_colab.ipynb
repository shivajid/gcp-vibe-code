{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAX MLP with Flax NNX on Colab TPU\n",
    "\n",
    "This notebook demonstrates how to define, train, and evaluate a Multi-Layer Perceptron (MLP) using JAX, Flax NNX, and Optax. It's designed to run on a Colab TPU environment.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Install necessary libraries.\n",
    "2. Import dependencies.\n",
    "3. Define the MLP model using `flax.nnx`.\n",
    "4. Generate synthetic classification data.\n",
    "5. Define training components: loss function, optimizer, and the training step function.\n",
    "6. Initialize the model and optimizer.\n",
    "7. Run the training loop.\n",
    "8. Evaluate the trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "First, we need to install the required Python packages: `jax` (and `jaxlib` compatible with the Colab TPU), `flax`, `optax`, and `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q jax jaxlib flax optax scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries\n",
    "\n",
    "Now, let's import all the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.nnx as nnx  # Using the stable nnx module\n",
    "import optax\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import functools\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"Flax version: {nnx.__version__}\")\n",
    "print(f\"Optax version: {optax.__version__}\")\n",
    "try:\n",
    "    print(f\"Detected TPUs: {jax.devices('tpu')}\")\n",
    "except RuntimeError:\n",
    "    print(\"No TPU detected. Ensure TPU runtime is selected in Colab: Runtime > Change runtime type > Hardware accelerator > TPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MLP Definition using Flax NNX\n",
    "\n",
    "We define our Multi-Layer Perceptron (MLP) model using `flax.nnx.Module`. NNX provides a more stateful and Pythonic way to define models compared to the traditional Flax functional core. \n",
    "\n",
    "- The `__init__` method constructs the layers (a series of hidden linear layers and an output linear layer). It takes the input size, a list of hidden layer sizes, and the output size as arguments. `nnx.Rngs` is used for initializing layer parameters.\n",
    "- The `__call__` method defines the forward pass of the model. Data flows through hidden layers with ReLU activations, and finally through the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nnx.Module):\n",
    "    def __init__(self, input_size: int, hidden_sizes: list[int], output_size: int, *, rngs: nnx.Rngs):\n",
    "        current_in_features = input_size\n",
    "        self.hidden_layers = []\n",
    "        for h_size in hidden_sizes:\n",
    "            layer = nnx.Linear(in_features=current_in_features, out_features=h_size, rngs=rngs)\n",
    "            self.hidden_layers.append(layer)\n",
    "            current_in_features = h_size\n",
    "        \n",
    "        self.output_layer = nnx.Linear(in_features=current_in_features, out_features=output_size, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x: jax.Array):\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "            x = jax.nn.relu(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Synthetic Data Generation\n",
    "\n",
    "This function generates a simple synthetic dataset for a classification task using `scikit-learn`'s `make_classification`. The labels are one-hot encoded for use with cross-entropy loss. The data is converted to JAX arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(n_samples=200, n_features=2, n_classes=2, random_state=42):\n",
    "    \"\"\"Generates simple synthetic data for classification.\"\"\"\n",
    "    X, y = make_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=n_features,\n",
    "        n_informative=n_features,\n",
    "        n_redundant=0,\n",
    "        n_repeated=0,\n",
    "        n_classes=n_classes,\n",
    "        n_clusters_per_class=1,\n",
    "        flip_y=0.01,\n",
    "        class_sep=1.0,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    y_one_hot = jax.nn.one_hot(y, num_classes=n_classes)\n",
    "    return jnp.array(X), jnp.array(y_one_hot), jnp.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop Components\n",
    "\n",
    "These functions define the core components needed for the training loop:\n",
    "\n",
    "### Loss Function (Cross-Entropy)\n",
    "Computes the cross-entropy loss, suitable for classification tasks. It uses `jax.nn.log_softmax` for numerical stability.\n",
    "\n",
    "### Optimizer (Adam)\n",
    "Returns an Adam optimizer instance from Optax with a specified learning rate.\n",
    "\n",
    "### Training Step Function (`train_step`)\n",
    "This function performs a single training step. It's JIT-compiled with `jax.jit` for performance.\n",
    "- **NNX State Handling**: Crucially, for Flax NNX, the model is split into trainable `params` and `static` parts. Inside the `loss_for_grad` function (which is differentiated), the model is temporarily reconstructed using `nnx.merge(current_params, static)` to perform the forward pass. This is because JAX transformations like `jax.grad` operate on functions of JAX arrays (the `params`).\n",
    "- **Gradient Calculation**: `jax.value_and_grad` computes both the loss value and the gradients of the loss with respect to the trainable parameters.\n",
    "- **Optimizer Update**: The Optax optimizer calculates parameter updates based on the gradients and its internal state.\n",
    "- **Parameter Application**: `optax.apply_updates` applies these updates to the model parameters.\n",
    "\n",
    "### Prediction Function (`predict`)\n",
    "Makes predictions using the trained model. Similar to `train_step`, it merges `params` and `static` parts to reconstruct the model before the forward pass. It's also JIT-compiled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function (Cross-Entropy for classification)\n",
    "def cross_entropy_loss(logits, labels_one_hot):\n",
    "    \"\"\"Computes cross-entropy loss.\"\"\"\n",
    "    return -jnp.sum(labels_one_hot * jax.nn.log_softmax(logits), axis=-1).mean()\n",
    "\n",
    "# Optimizer\n",
    "def get_optimizer(learning_rate=1e-3):\n",
    "    return optax.adam(learning_rate)\n",
    "\n",
    "# Training Step Function\n",
    "@functools.partial(jax.jit, static_argnames=('loss_fn', 'optimizer_update_fn'))\n",
    "def train_step(params, static, opt_state, loss_fn, optimizer_update_fn, X_batch, y_batch):\n",
    "    \"\"\"Performs a single training step with Flax NNX using split state.\"\"\"\n",
    "    def loss_for_grad(current_params):\n",
    "        # Reconstruct the model for the forward pass\n",
    "        model_for_forward_pass = nnx.merge(current_params, static)\n",
    "        logits = model_for_forward_pass(X_batch) # Call the model directly\n",
    "        return loss_fn(logits, y_batch)\n",
    "\n",
    "    loss_val, grads = jax.value_and_grad(loss_for_grad)(params) # Grads w.r.t. params\n",
    "    updates, new_opt_state = optimizer_update_fn(grads, opt_state, params) # Pass params for optimizer\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, new_opt_state, loss_val\n",
    "\n",
    "# Prediction function\n",
    "@jax.jit\n",
    "def predict(params, static, X):\n",
    "    \"\"\"Makes predictions using the model with split state.\"\"\"\n",
    "    model_for_prediction = nnx.merge(params, static)\n",
    "    return model_for_prediction(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Main Execution: Configuration, Initialization, Training, and Evaluation\n",
    "\n",
    "This section brings everything together:\n",
    "1.  **Configuration**: Define hyperparameters like dataset size, features, classes, network architecture, learning rate, epochs, and batch size.\n",
    "2.  **PRNG Keys**: JAX requires explicit management of pseudo-random number generator (PRNG) keys. We create and split keys for data generation, model initialization, and shuffling within the training loop.\n",
    "3.  **Data Preparation**: Generate the synthetic dataset and split it into training and testing sets.\n",
    "4.  **Model and Optimizer Initialization**:\n",
    "    - Create an instance of the `MLP` model. `nnx.Rngs(params=key_init)` provides the necessary PRNG key for parameter initialization within the NNX model.\n",
    "    - Perform a \"dry run\" by passing a dummy input through the model. This ensures all layers are built and parameters are initialized before splitting the model state.\n",
    "    - Split the NNX model into trainable `params` (e.g., weights, biases) and `static` parts (e.g., layer structure, non-trainable attributes) using `nnx.split(model)`.\n",
    "    - Initialize the Optax optimizer with the trainable `params`.\n",
    "5.  **Training Loop**:\n",
    "    - Iterate for the specified number of epochs.\n",
    "    - In each epoch, shuffle the training data.\n",
    "    - Iterate over batches of the shuffled data.\n",
    "    - Call the `train_step` function to update model parameters and optimizer state for each batch.\n",
    "    - Accumulate and print the average loss periodically.\n",
    "6.  **Evaluation**:\n",
    "    - After training, use the `predict` function to get logits for the test set.\n",
    "    - Calculate the classification accuracy by comparing the predicted class labels (derived from `argmax` of logits) with the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "N_SAMPLES = 500\n",
    "N_FEATURES = 4\n",
    "N_CLASSES = 3\n",
    "HIDDEN_SIZES = [64, 32]\n",
    "LEARNING_RATE = 0.005\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "PRINT_EVERY_EPOCHS = 10\n",
    "SEED = 42\n",
    "\n",
    "key = jax.random.PRNGKey(SEED)\n",
    "key_data, key_init, key_shuffle = jax.random.split(key, 3)\n",
    "\n",
    "# Generate Data\n",
    "X_data, y_data_one_hot, y_data_orig = generate_synthetic_data(\n",
    "    n_samples=N_SAMPLES, n_features=N_FEATURES, n_classes=N_CLASSES, random_state=int(jax.random.key_data_bits(key_data)[0]) # Use key for sklearn too\n",
    ")\n",
    "X_train, X_test, y_train_one_hot, y_test_one_hot, y_train_orig, y_test_orig = train_test_split(\n",
    "    X_data, y_data_one_hot, y_data_orig, test_size=0.2, random_state=SEED # sklearn's random_state\n",
    ")\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}, y_train_one_hot shape: {y_train_one_hot.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test_one_hot shape: {y_test_one_hot.shape}\")\n",
    "print(f\"Number of classes: {N_CLASSES}\")\n",
    "\n",
    "# Initialize Model and Optimizer with Flax NNX\n",
    "# Create a dummy input to infer shapes\n",
    "dummy_x = X_train[:1]\n",
    "\n",
    "# NNX model initialization\n",
    "model = MLP(input_size=N_FEATURES, hidden_sizes=HIDDEN_SIZES, output_size=N_CLASSES, rngs=nnx.Rngs(params=key_init))\n",
    "\n",
    "# Initialize parameters by a \"dry run\"\n",
    "_ = model(dummy_x) \n",
    "\n",
    "# Split the model into trainable parameters and static parts\n",
    "params, static = nnx.split(model)\n",
    "\n",
    "optimizer = get_optimizer(learning_rate=LEARNING_RATE)\n",
    "opt_state = optimizer.init(params) # Optimizer initializes with trainable parameters\n",
    "\n",
    "# Training Loop\n",
    "num_train_samples = X_train.shape[0]\n",
    "num_batches = num_train_samples // BATCH_SIZE\n",
    "\n",
    "print(f\"\\nStarting training for {EPOCHS} epochs...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    key_shuffle, key_loop = jax.random.split(key_shuffle) # Get a new key for this epoch's shuffle\n",
    "    permutation = jax.random.permutation(key_loop, num_train_samples)\n",
    "    shuffled_X_train = X_train[permutation]\n",
    "    shuffled_y_train_one_hot = y_train_one_hot[permutation]\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * BATCH_SIZE\n",
    "        end_idx = start_idx + BATCH_SIZE\n",
    "        X_batch = shuffled_X_train[start_idx:end_idx]\n",
    "        y_batch = shuffled_y_train_one_hot[start_idx:end_idx]\n",
    "\n",
    "        params, opt_state, loss_val = train_step(\n",
    "            params,         \n",
    "            static,         \n",
    "            opt_state,\n",
    "            cross_entropy_loss, \n",
    "            optimizer.update,   \n",
    "            X_batch,\n",
    "            y_batch\n",
    "        )\n",
    "        epoch_loss += loss_val\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / num_batches\n",
    "    if (epoch + 1) % PRINT_EVERY_EPOCHS == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}, Avg Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# Evaluation (simple accuracy)\n",
    "test_logits = predict(params, static, X_test)\n",
    "predicted_classes = jnp.argmax(test_logits, axis=1)\n",
    "accuracy = jnp.mean(predicted_classes == y_test_orig)\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "This notebook demonstrated the process of building, training, and evaluating an MLP using JAX with the Flax NNX API and Optax for optimization. The use of `nnx.split` and `nnx.merge` is key to integrating the stateful NNX model style with JAX's functional transformations and Optax's parameter-based optimization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.x.y"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}